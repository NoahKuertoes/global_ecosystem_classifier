{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DB SIGN IN: ---\n",
      "reading .env files...\n",
      "successfully connected to:\t postgres\n",
      "available schemas:\t\t ecosystem_classifier, information_schema, public\n",
      "setting default schema to:\t ecosystem_classifier\n",
      "tables in default schema:\t radiance_2020, ndvi_2020, earthdata_1980, meteostat_2017, us_rules, meteostat_2010, meteostat_2012, meteostat_2013, meteostat_2014, meteostat_2018, meteostat_2000, elevation, us_gaz, earthdata_1990, earthdata_1995, us_lex, meteostat_2015, earthdata_2011, earthdata_2012, meteostat_1990, meteostat_2011, meteostat_stations, earthdata_1970, earthdata_2019, radiance_2017, radiance_2018, radiance_2019, radiance_2021, earthdata_2000, earthdata_2005, earthdata_2010, earthdata_2013, meteostat_2016, spatial_ref_sys, app_contact_messages, earthdata_2014, meteostat_2019, meteostat_2020, meteostat_2021, earthdata_2015, earthdata_2016, earthdata_2017, earthdata_2018, meteostat_2005, radiance_2012, radiance_2013, radiance_2014, radiance_2015, radiance_2016, radiance_2022, radiance_2023, meteostat_1995, earthdata_1960, earthdata_2021, earthdata_2022, earthdata_2023, earthdata_2020, earthdata_1948, earthdata_1950\n",
      "lat_min:\t -2\n",
      "lat_max:\t 2\n",
      "lon_min:\t -60\n",
      "lon_max:\t -58\n",
      "years:\t [2020]\n",
      ">>> Fitting tables: elevation, radiance_2020, ndvi_2020, meteostat_2020, earthdata_2020\n",
      "\n",
      "--- CHECKING TABLES: ---\n",
      "KEPT:\t\televation\n",
      "KEPT:\t\tradiance_2020\n",
      "KEPT:\t\tndvi_2020\n",
      "DISCARDED:\tmeteostat_2020 >>> does not feature 'lon/longitude' and 'lat/latitude' as columns\n",
      "KEPT:\t\tearthdata_2020\n",
      "\n",
      "--- FETCHING DATA: ---\n",
      ">>> SUCCESS: elevation data fetched\n",
      ">>> SUCCESS: radiance_2020 data fetched\n",
      ">>> SUCCESS: ndvi_2020 data fetched\n",
      ">>> SUCCESS: earthdata_2020 data fetched\n"
     ]
    }
   ],
   "source": [
    "from fetch_data import fetch_data_from_postgres\n",
    "#Merge data\n",
    "mvariables = {\n",
    "    \"lat_min\": -2,\n",
    "    \"lat_max\": 2,\n",
    "    \"lon_min\": -60,\n",
    "    \"lon_max\": -58,\n",
    "    \"years\": [2020]\n",
    "}\n",
    "\n",
    "json_outputs = fetch_data_from_postgres(variables=mvariables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged DataFrame:\n",
      "     lat     lon  elevation  radiance      ndvi  Swnet_min  Swnet_max  \\\n",
      "0 -1.875 -59.875       97.0  0.005382  0.721918  165.80704  229.59800   \n",
      "1 -1.875 -59.875       97.0  0.005382  0.721918  165.80704  229.59800   \n",
      "2 -1.875 -59.625       81.0       NaN  0.478903  166.42150  227.04250   \n",
      "3 -1.875 -59.375       38.0  0.038720  0.557888  153.92062  221.87888   \n",
      "4 -1.875 -59.375       38.0  0.038720  0.557888  153.92062  221.87888   \n",
      "5 -1.875 -59.125      106.0       NaN  0.759872  158.95009  226.96942   \n",
      "6 -1.875 -58.875      137.0       NaN  0.775727  153.02817  227.91917   \n",
      "7 -1.875 -58.625       73.0       NaN  0.776553  152.32771  220.99000   \n",
      "8 -1.875 -58.375       42.0       NaN  0.772459  150.33238  218.94513   \n",
      "9 -1.875 -58.125       72.0       NaN  0.813295  148.02092  215.03604   \n",
      "\n",
      "   Swnet_avg  Lwnet_min  Lwnet_max  ...  Qair_avg  Psurf_min   Psurf_max  \\\n",
      "0  193.64354 -53.923150 -31.744638  ...  0.016727   99407.14   99696.740   \n",
      "1  193.64354 -53.923150 -31.744638  ...  0.016727   99407.14   99696.740   \n",
      "2  191.48387 -53.959354 -30.794430  ...  0.016663   99905.94  100200.230   \n",
      "3  182.97272 -54.305230 -28.496597  ...  0.016567   99789.77  100082.410   \n",
      "4  182.97272 -54.305230 -28.496597  ...  0.016567   99789.77  100082.410   \n",
      "5  186.06682 -54.740900 -29.459680  ...  0.016416   99278.99   99568.414   \n",
      "6  185.30913 -58.529730 -30.398514  ...  0.016381   99367.62   99658.280   \n",
      "7  182.85860 -57.996940 -29.579556  ...  0.016332   99586.53   99882.570   \n",
      "8  182.30817 -58.956856 -28.719970  ...  0.016316   99997.58  100301.695   \n",
      "9  183.11378 -59.845814 -28.606388  ...  0.016261   99802.01  100104.914   \n",
      "\n",
      "    Psurf_avg  SWdown_min  SWdown_max  SWdown_avg  LWdown_min  LWdown_max  \\\n",
      "0   99540.440   182.20546   252.30525   212.79494   399.87967   418.04547   \n",
      "1   99540.440   182.20546   252.30525   212.79494   399.87967   418.04547   \n",
      "2  100040.914   181.88155   248.13367   209.27197   402.52460   420.99180   \n",
      "3   99923.870   169.14350   243.82283   201.06883   402.56458   422.08070   \n",
      "4   99923.870   169.14350   243.82283   201.06883   402.56458   422.08070   \n",
      "5   99413.730   174.67004   249.41670   204.46892   399.68950   419.55524   \n",
      "6   99503.780   168.16275   250.46054   203.63649   398.03854   419.68353   \n",
      "7   99727.560   167.39296   242.84645   200.94359   398.63284   421.29535   \n",
      "8  100144.445   165.20058   240.59912   200.33867   400.26004   421.99160   \n",
      "9   99950.440   162.66025   236.30342   201.22389   399.69205   423.29337   \n",
      "\n",
      "   LWdown_avg  \n",
      "0   410.70316  \n",
      "1   410.70316  \n",
      "2   413.96652  \n",
      "3   414.65170  \n",
      "4   414.65170  \n",
      "5   412.11145  \n",
      "6   411.29538  \n",
      "7   412.35934  \n",
      "8   413.54135  \n",
      "9   413.07740  \n",
      "\n",
      "[10 rows x 113 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop over each table in json_outputs\n",
    "for key, data in json_outputs.items():\n",
    "    # Parse JSON string if necessary\n",
    "    if isinstance(data, str):\n",
    "        try:\n",
    "            parsed_data = json.loads(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for table '{key}': {e}\")\n",
    "            continue  # Skip this table if JSON parsing fails\n",
    "    else:\n",
    "        parsed_data = data\n",
    "\n",
    "    # Convert parsed data to a DataFrame\n",
    "    try:\n",
    "        df = pd.DataFrame(parsed_data)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error creating DataFrame for table '{key}': {e}\")\n",
    "        continue  # Skip this table if DataFrame creation fails\n",
    "\n",
    "    # Standardize column names to 'lat' and 'lon' for merging\n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        df.rename(columns={'latitude': 'lat', 'longitude': 'lon'}, inplace=True)\n",
    "    elif 'lat' not in df.columns or 'lon' not in df.columns:\n",
    "        print(f\"Warning: Table '{key}' is missing required 'lat'/'lon' or 'latitude'/'longitude' columns.\")\n",
    "        continue  # Skip tables without valid latitude/longitude columns\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Merge all DataFrames on 'lat' and 'lon' if there are valid DataFrames\n",
    "if dfs:\n",
    "    merged_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        try:\n",
    "            merged_df = merged_df.merge(df, on=['lat', 'lon'], how='outer')\n",
    "        except KeyError as e:\n",
    "            print(f\"Error merging table due to missing 'lat'/'lon' columns: {e}\")\n",
    "            continue  # Skip merging if 'lat'/'lon' columns are missing\n",
    "\n",
    "    # Display the merged DataFrame\n",
    "    print(\"\\nMerged DataFrame:\")\n",
    "    print(merged_df.head(10))\n",
    "else:\n",
    "    print(\"No valid tables with latitude and longitude data were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"ecosystem\"] = \"Urban-City\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lwnet_avg</th>\n",
       "      <th>Qle_min</th>\n",
       "      <th>Qle_max</th>\n",
       "      <th>Qle_avg</th>\n",
       "      <th>Qh_min</th>\n",
       "      <th>Qh_max</th>\n",
       "      <th>Qh_avg</th>\n",
       "      <th>Qg_min</th>\n",
       "      <th>Qg_max</th>\n",
       "      <th>Qg_avg</th>\n",
       "      <th>Snowf_min</th>\n",
       "      <th>Snowf_max</th>\n",
       "      <th>Snowf_avg</th>\n",
       "      <th>Rainf_min</th>\n",
       "      <th>Rainf_max</th>\n",
       "      <th>Rainf_avg</th>\n",
       "      <th>Evap_min</th>\n",
       "      <th>Evap_max</th>\n",
       "      <th>Evap_avg</th>\n",
       "      <th>Qs_acc_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41.843624</td>\n",
       "      <td>109.06175</td>\n",
       "      <td>152.98096</td>\n",
       "      <td>125.803246</td>\n",
       "      <td>10.713118</td>\n",
       "      <td>38.366722</td>\n",
       "      <td>26.437057</td>\n",
       "      <td>-0.807152</td>\n",
       "      <td>0.618713</td>\n",
       "      <td>-0.236860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.005627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-41.843624</td>\n",
       "      <td>109.06175</td>\n",
       "      <td>152.98096</td>\n",
       "      <td>125.803246</td>\n",
       "      <td>10.713118</td>\n",
       "      <td>38.366722</td>\n",
       "      <td>26.437057</td>\n",
       "      <td>-0.807152</td>\n",
       "      <td>0.618713</td>\n",
       "      <td>-0.236860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.005627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-40.746574</td>\n",
       "      <td>109.37356</td>\n",
       "      <td>154.90436</td>\n",
       "      <td>126.680640</td>\n",
       "      <td>14.367916</td>\n",
       "      <td>34.853592</td>\n",
       "      <td>24.474670</td>\n",
       "      <td>-0.794986</td>\n",
       "      <td>0.745487</td>\n",
       "      <td>-0.178876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.006062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lwnet_avg    Qle_min    Qle_max     Qle_avg     Qh_min     Qh_max  \\\n",
       "0 -41.843624  109.06175  152.98096  125.803246  10.713118  38.366722   \n",
       "1 -41.843624  109.06175  152.98096  125.803246  10.713118  38.366722   \n",
       "2 -40.746574  109.37356  154.90436  126.680640  14.367916  34.853592   \n",
       "\n",
       "      Qh_avg    Qg_min    Qg_max    Qg_avg  Snowf_min  Snowf_max  Snowf_avg  \\\n",
       "0  26.437057 -0.807152  0.618713 -0.236860        0.0        0.0        0.0   \n",
       "1  26.437057 -0.807152  0.618713 -0.236860        0.0        0.0        0.0   \n",
       "2  24.474670 -0.794986  0.745487 -0.178876        0.0        0.0        0.0   \n",
       "\n",
       "   Rainf_min  Rainf_max  Rainf_avg  Evap_min  Evap_max  Evap_avg  Qs_acc_min  \n",
       "0   0.000028   0.000133   0.000070  0.000044  0.000061  0.000050    0.005627  \n",
       "1   0.000028   0.000133   0.000070  0.000044  0.000061  0.000050    0.005627  \n",
       "2   0.000029   0.000135   0.000069  0.000044  0.000062  0.000051    0.006062  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.iloc[0:3,10:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
