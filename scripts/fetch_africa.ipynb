{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DB SIGN IN: ---\n",
      "reading .env files...\n",
      "successfully connected to:\t postgres\n",
      "available schemas:\t\t ecosystem_classifier, information_schema, public\n",
      "setting default schema to:\t ecosystem_classifier\n",
      "tables in default schema:\t radiance_2020, earthdata_1980, meteostat_2017, us_rules, meteostat_2010, meteostat_2012, meteostat_2013, meteostat_2014, meteostat_2018, elevation, us_gaz, earthdata_1990, earthdata_1995, us_lex, meteostat_2015, earthdata_2011, earthdata_2012, meteostat_2011, meteostat_stations, earthdata_1970, earthdata_2019, earthdata_2000, earthdata_2005, earthdata_2010, earthdata_2013, meteostat_2016, spatial_ref_sys, app_contact_messages, earthdata_2014, meteostat_2019, meteostat_2020, meteostat_2021, earthdata_2015, earthdata_2016, earthdata_2017, earthdata_2018, meteostat_2005, earthdata_1960, earthdata_2021, earthdata_2022, earthdata_2023, earthdata_2020, earthdata_1948, earthdata_1950\n",
      "\n",
      "--- READING FETCHING PARAMETERS: ---\n",
      "lat_min:\t 3\n",
      "lat_max:\t 15\n",
      "lon_min:\t -20\n",
      "lon_max:\t 52\n",
      "years:\t [2020]\n",
      ">>> Fitting tables: elevation, radiance_2020, meteostat_2020, earthdata_2020\n",
      "\n",
      "--- CHECKING TABLES: ---\n",
      "KEPT:\t\televation\n",
      "KEPT:\t\tradiance_2020\n",
      "DISCARDED:\tmeteostat_2020 >>> does not feature 'lon/longitude' and 'lat/latitude' as columns\n",
      "KEPT:\t\tearthdata_2020\n",
      "\n",
      "--- FETCHING DATA: ---\n",
      ">>> SUCCESS: elevation data fetched\n",
      ">>> SUCCESS: radiance_2020 data fetched\n",
      ">>> SUCCESS: earthdata_2020 data fetched\n"
     ]
    }
   ],
   "source": [
    "#Merge data\n",
    "json_outputs = fetch_data_from_postgres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged DataFrame:\n",
      "     lat     lon  elevation  radiance  Swnet_min  Swnet_max  Swnet_avg  \\\n",
      "0  3.125   6.625        NaN  0.186914        NaN        NaN        NaN   \n",
      "1  3.125   6.875        NaN  0.221774        NaN        NaN        NaN   \n",
      "2  3.125   9.875        NaN  0.126922        NaN        NaN        NaN   \n",
      "3  3.125  10.125       40.0  0.012757  151.43755  203.05194  171.92729   \n",
      "4  3.125  10.375      110.0       NaN  152.86908  203.03526  171.89952   \n",
      "5  3.125  10.625      362.0       NaN  152.27028  206.41410  175.54732   \n",
      "6  3.125  10.875      633.0       NaN  154.93806  207.57379  178.19983   \n",
      "7  3.125  11.125      831.0       NaN  157.89967  212.25233  183.51044   \n",
      "8  3.125  11.375      703.0       NaN  156.04794  209.56664  181.43266   \n",
      "9  3.125  11.625      716.0       NaN  156.02440  207.93358  180.82967   \n",
      "\n",
      "   Lwnet_min  Lwnet_max  Lwnet_avg  ...  Qair_avg  Psurf_min   Psurf_max  \\\n",
      "0        NaN        NaN        NaN  ...       NaN        NaN         NaN   \n",
      "1        NaN        NaN        NaN  ...       NaN        NaN         NaN   \n",
      "2        NaN        NaN        NaN  ...       NaN        NaN         NaN   \n",
      "3 -70.759964 -37.485650 -47.466870  ...  0.017653  100003.52  100318.600   \n",
      "4 -70.905396 -37.761696 -47.831127  ...  0.017423   98388.29   98682.445   \n",
      "5 -71.992836 -36.817340 -48.371582  ...  0.016879   95881.45   96134.490   \n",
      "6 -73.181180 -36.879803 -48.468140  ...  0.016382   94143.04   94371.210   \n",
      "7 -70.649765 -32.262260 -44.581944  ...  0.015925   93098.99   93319.170   \n",
      "8 -72.663925 -33.345770 -45.605480  ...  0.015578   93284.91   93511.280   \n",
      "9 -74.111340 -33.740932 -46.432285  ...  0.015251   93348.34   93579.766   \n",
      "\n",
      "    Psurf_avg  SWdown_min  SWdown_max  SWdown_avg  LWdown_min  LWdown_max  \\\n",
      "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "3  100158.130   173.56508   235.65056   195.88374   396.59440   414.26157   \n",
      "4   98533.125   173.59927   235.77922   196.37325   390.47070   410.24695   \n",
      "5   96007.290   171.51706   234.40940   198.15670   382.35712   403.21250   \n",
      "6   94256.480   174.08770   233.22914   200.22443   376.48962   398.10898   \n",
      "7   93206.220   177.41536   238.48538   206.19147   379.20032   402.01364   \n",
      "8   93395.020   175.33484   235.46767   203.85698   378.41327   402.63220   \n",
      "9   93461.710   175.30827   233.63332   203.17940   376.00220   402.52330   \n",
      "\n",
      "   LWdown_avg  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3   406.41590  \n",
      "4   401.68857  \n",
      "5   394.58838  \n",
      "6   389.94950  \n",
      "7   394.14273  \n",
      "8   395.06583  \n",
      "9   394.79883  \n",
      "\n",
      "[10 rows x 112 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop over each table in json_outputs\n",
    "for key, data in json_outputs.items():\n",
    "    # Parse JSON string if necessary\n",
    "    if isinstance(data, str):\n",
    "        try:\n",
    "            parsed_data = json.loads(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for table '{key}': {e}\")\n",
    "            continue  # Skip this table if JSON parsing fails\n",
    "    else:\n",
    "        parsed_data = data\n",
    "\n",
    "    # Convert parsed data to a DataFrame\n",
    "    try:\n",
    "        df = pd.DataFrame(parsed_data)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error creating DataFrame for table '{key}': {e}\")\n",
    "        continue  # Skip this table if DataFrame creation fails\n",
    "\n",
    "    # Standardize column names to 'lat' and 'lon' for merging\n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        df.rename(columns={'latitude': 'lat', 'longitude': 'lon'}, inplace=True)\n",
    "    elif 'lat' not in df.columns or 'lon' not in df.columns:\n",
    "        print(f\"Warning: Table '{key}' is missing required 'lat'/'lon' or 'latitude'/'longitude' columns.\")\n",
    "        continue  # Skip tables without valid latitude/longitude columns\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Merge all DataFrames on 'lat' and 'lon' if there are valid DataFrames\n",
    "if dfs:\n",
    "    merged_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        try:\n",
    "            merged_df = merged_df.merge(df, on=['lat', 'lon'], how='outer')\n",
    "        except KeyError as e:\n",
    "            print(f\"Error merging table due to missing 'lat'/'lon' columns: {e}\")\n",
    "            continue  # Skip merging if 'lat'/'lon' columns are missing\n",
    "\n",
    "    # Display the merged DataFrame\n",
    "    print(\"\\nMerged DataFrame:\")\n",
    "    print(merged_df.head(10))\n",
    "else:\n",
    "    print(\"No valid tables with latitude and longitude data were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11220, 112)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              NaN\n",
       "1              NaN\n",
       "2              NaN\n",
       "3        12.185192\n",
       "4        12.413850\n",
       "           ...    \n",
       "11215    19.827429\n",
       "11216    19.704557\n",
       "11217    19.624940\n",
       "11218    19.249939\n",
       "11219          NaN\n",
       "Name: Albedo_avg, Length: 11220, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"Albedo_avg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat\n",
      "lon\n",
      "elevation\n",
      "Swnet_min\n",
      "Swnet_max\n",
      "Swnet_avg\n",
      "Lwnet_min\n",
      "Lwnet_max\n",
      "Lwnet_avg\n",
      "Qle_min\n",
      "Qle_max\n",
      "Qle_avg\n",
      "Qh_min\n",
      "Qh_max\n",
      "Qh_avg\n",
      "Qg_min\n",
      "Qg_max\n",
      "Qg_avg\n",
      "Snowf_min\n",
      "Snowf_max\n",
      "Snowf_avg\n",
      "Rainf_min\n",
      "Rainf_max\n",
      "Rainf_avg\n",
      "Evap_min\n",
      "Evap_max\n",
      "Evap_avg\n",
      "Qs_acc_min\n",
      "Qs_acc_max\n",
      "Qs_acc_avg\n",
      "Qsb_acc_min\n",
      "Qsb_acc_max\n",
      "Qsb_acc_avg\n",
      "Qsm_acc_min\n",
      "Qsm_acc_max\n",
      "Qsm_acc_avg\n",
      "AvgSurfT_min\n",
      "AvgSurfT_max\n",
      "AvgSurfT_avg\n",
      "Albedo_min\n",
      "Albedo_max\n",
      "Albedo_avg\n",
      "SWE_min\n",
      "SWE_max\n",
      "SWE_avg\n",
      "SnowDepth_min\n",
      "SnowDepth_max\n",
      "SnowDepth_avg\n",
      "SoilM010_min\n",
      "SoilM010_max\n",
      "SoilM010_avg\n",
      "SoilM1040_min\n",
      "SoilM1040_max\n",
      "SoilM1040_avg\n",
      "SoilM40100_min\n",
      "SoilM40100_max\n",
      "SoilM40100_avg\n",
      "SoilM100200_min\n",
      "SoilM100200_max\n",
      "SoilM100200_avg\n",
      "SoilT010_min\n",
      "SoilT010_max\n",
      "SoilT010_avg\n",
      "SoilT1040_min\n",
      "SoilT1040_max\n",
      "SoilT1040_avg\n",
      "SoilT40100_min\n",
      "SoilT40100_max\n",
      "SoilT40100_avg\n",
      "SoilT100200_min\n",
      "SoilT100200_max\n",
      "SoilT100200_avg\n",
      "PotEvap_min\n",
      "PotEvap_max\n",
      "PotEvap_avg\n",
      "ECanop_min\n",
      "ECanop_max\n",
      "ECanop_avg\n",
      "Tveg_min\n",
      "Tveg_max\n",
      "Tveg_avg\n",
      "ESoil_min\n",
      "ESoil_max\n",
      "ESoil_avg\n",
      "RootMoist_min\n",
      "RootMoist_max\n",
      "RootMoist_avg\n",
      "CanopInt_min\n",
      "CanopInt_max\n",
      "CanopInt_avg\n",
      "Wind_min\n",
      "Wind_max\n",
      "Wind_avg\n",
      "Rainf_min.1\n",
      "Rainf_max.1\n",
      "Rainf_avg.1\n",
      "Tair_min\n",
      "Tair_max\n",
      "Tair_avg\n",
      "Qair_min\n",
      "Qair_max\n",
      "Qair_avg\n",
      "Psurf_min\n",
      "Psurf_max\n",
      "Psurf_avg\n",
      "SWdown_min\n",
      "SWdown_max\n",
      "SWdown_avg\n",
      "LWdown_min\n",
      "LWdown_max\n",
      "LWdown_avg\n"
     ]
    }
   ],
   "source": [
    "for col in merged_df.columns:\n",
    "    print(col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
